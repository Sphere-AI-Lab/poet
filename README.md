# POET

This repository contains the official implementation for the paper:

**Reparameterized LLM Training via Orthogonal Equivalence Transformation** ([paper](https://arxiv.org)).


## Status

ðŸš§ **Work in Progress** ðŸš§

This codebase is currently under development. The implementation will be made available soon.


## Citation

If you find this work useful in your research, please cite our paper:

```bibtex
@article{reparameterized_llm_training,
  title={Reparameterized LLM Training via Orthogonal Equivalence Transformation},
  author={[Authors]},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```
